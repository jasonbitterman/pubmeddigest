{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from Bio import Entrez\n",
    "import time\n",
    "\n",
    "# Define topic for summary document. Don't be too broad or you will retrieve a very large number of papers.\n",
    "topic = \"lumbar facet joint\"  # THOUGHTS - could use LLM to refine topic?\n",
    "topics = [topic]\n",
    "\n",
    "# Define date ranges\n",
    "start_date = \"2024/12/01\"\n",
    "end_date = \"2025/07/05\"\n",
    "date_range = f'(\"{start_date}\"[Date - Create] : \"{end_date}\"[Date - Create])'\n",
    "\n",
    "# Define max number of results to return.\n",
    "# If you plan to search for more than about 15 articles, you will need to create your own Entrez account and generate an API key and enter them below.\n",
    "max_results = 30\n",
    "\n",
    "# Enter your Entrez account email address abd API key. If you plan on only summarizing less than about 15 articles, you can leave these empty.\n",
    "Entrez.email = \"jasonbitt@gmail.com\"  # Enter your email address\n",
    "Entrez.api_key = \"a21f5cd33f0e3f0f0730d4562ebdacdefb09\"  # Enter your Entrez API key (can be generated at https://account.ncbi.nlm.nih.gov/settings/)\n",
    "\n",
    "# Build the query dynamically based on the available topics\n",
    "queries = []\n",
    "\n",
    "if topics:\n",
    "    topic_queries = [\"{}[Title/Abstract]\".format(topic) for topic in topics]\n",
    "    queries.append(\"(\" + \" OR \".join(topic_queries) + \")\")\n",
    "\n",
    "full_query = \" AND \".join(queries) + \" AND \" + date_range\n",
    "\n",
    "# Search PubMed for relevant records\n",
    "handle = Entrez.esearch(db=\"pubmed\", retmax=max_results, term=full_query)\n",
    "record = Entrez.read(handle)\n",
    "id_list = record[\"IdList\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Count': '18', 'RetMax': '18', 'RetStart': '0', 'IdList': ['40608369', '40525005', '40493053', '40473823', '40469068', '40391994', '40375248', '40342788', '40340789', '40330008', '40241056', '40221590', '40199535', '39971339', '39869888', '39781088', '39734603', '39722208'], 'TranslationSet': [], 'QueryTranslation': '\"lumbar facet joint\"[Title/Abstract] AND 2024/12/01:2025/07/05[Date - Create]'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from Bio import Entrez\n",
    "import time\n",
    "\n",
    "\n",
    "# DataFrame to store the extracted data\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"PMID\",\n",
    "        \"Title\",\n",
    "        \"Abstract\",\n",
    "        \"Journal\",\n",
    "        \"Keywords\",\n",
    "        \"URL\",\n",
    "        \"PubDate\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fetch information for each record in the id_list\n",
    "for pmid in id_list:\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=pmid, retmode=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "\n",
    "    # Process each PubMed article in the response\n",
    "    for record in records[\"PubmedArticle\"]:\n",
    "        # Print the record in a formatted JSON style\n",
    "        # print(\n",
    "        # json.dumps(record, indent=4, default=str)\n",
    "        # )  # default=str handles types JSON can't serialize like datetime\n",
    "\n",
    "        article = record[\"MedlineCitation\"][\"Article\"]\n",
    "\n",
    "        title = article[\"ArticleTitle\"]\n",
    "        abstract = (\n",
    "            \" \".join(article[\"Abstract\"][\"AbstractText\"])\n",
    "            if \"Abstract\" in article and \"AbstractText\" in article[\"Abstract\"]\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "        journal_title = article[\"Journal\"][\"Title\"]\n",
    "        keywords = (\n",
    "            \", \".join(\n",
    "                keyword[\"DescriptorName\"]\n",
    "                for keyword in record[\"MedlineCitation\"][\"MeshHeadingList\"]\n",
    "            )\n",
    "            if \"MeshHeadingList\" in record[\"MedlineCitation\"]\n",
    "            else \"\"\n",
    "        )\n",
    "        url = f\"https://www.ncbi.nlm.nih.gov/pubmed/{pmid}\"\n",
    "        pub_date = article[\"Journal\"][\"JournalIssue\"][\"PubDate\"]\n",
    "\n",
    "        new_row = pd.DataFrame(\n",
    "            {\n",
    "                \"PMID\": [pmid],\n",
    "                \"Title\": [title],\n",
    "                \"Abstract\": [abstract],\n",
    "                \"Journal\": [journal_title],\n",
    "                \"Keywords\": [keywords],\n",
    "                \"URL\": [url],\n",
    "                \"PubDate\": [pub_date],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    time.sleep(0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PubDate to standardized MM/DD/YYYY format\n",
    "def standardize_date(pub_date):\n",
    "    year = pub_date.get(\"Year\", \"\")\n",
    "    month = pub_date.get(\"Month\", \"01\")\n",
    "    day = pub_date.get(\"Day\", \"01\")\n",
    "\n",
    "    # Convert month name to number if needed\n",
    "    month_map = {\n",
    "        \"Jan\": \"01\",\n",
    "        \"Feb\": \"02\",\n",
    "        \"Mar\": \"03\",\n",
    "        \"Apr\": \"04\",\n",
    "        \"May\": \"05\",\n",
    "        \"Jun\": \"06\",\n",
    "        \"Jul\": \"07\",\n",
    "        \"Aug\": \"08\",\n",
    "        \"Sep\": \"09\",\n",
    "        \"Oct\": \"10\",\n",
    "        \"Nov\": \"11\",\n",
    "        \"Dec\": \"12\",\n",
    "    }\n",
    "    if month in month_map:\n",
    "        month = month_map[month]\n",
    "\n",
    "    # Ensure month and day are 2 digits\n",
    "    month = month.zfill(2)\n",
    "    day = str(day).zfill(2)\n",
    "\n",
    "    return f\"{month}/{day}/{year}\"\n",
    "\n",
    "\n",
    "# Apply the date standardization to the PubDate column\n",
    "df[\"PubDate\"] = df[\"PubDate\"].apply(standardize_date)\n",
    "\n",
    "# Create markdown content\n",
    "markdown_content = \"# PubMed Search Results\\n\\n\"\n",
    "\n",
    "for i, (_, row) in enumerate(df.iterrows(), 1):\n",
    "    markdown_content += f\"## {i}. {row['Title']}\\n\\n\"\n",
    "    markdown_content += f\"**Journal:** {row['Journal']}\\n\\n\"\n",
    "    markdown_content += f\"**Publication Date:** {row['PubDate']}\\n\\n\"\n",
    "    markdown_content += f\"**Keywords:** {row['Keywords']}\\n\\n\"\n",
    "    markdown_content += f\"**Abstract:**\\n{row['Abstract']}\\n\\n\"\n",
    "    markdown_content += f\"**URL:** [{row['URL']})\\n\\n\"\n",
    "    markdown_content += f\"**PMID:** [{row['PMID']}]\\n\\n\"\n",
    "    markdown_content += \"---\\n\\n\"\n",
    "\n",
    "# Save to markdown file\n",
    "with open(\"PubMed_results.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(markdown_content)\n",
    "file_name = \"PubMed_results.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basics for LLM (OpenAI)\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-ACnsQWR55VdN35f73pdnvQ\n",
      "vs_68783b47528881918725716e4f7e47e4\n",
      "\n",
      "\n",
      "SyncCursorPage[VectorStoreFile](data=[VectorStoreFile(id='file-ACnsQWR55VdN35f73pdnvQ', created_at=1752709961, last_error=None, object='vector_store.file', status='in_progress', usage_bytes=0, vector_store_id='vs_68783b47528881918725716e4f7e47e4', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))], has_more=False, object='list', first_id='file-ACnsQWR55VdN35f73pdnvQ', last_id='file-ACnsQWR55VdN35f73pdnvQ')\n"
     ]
    }
   ],
   "source": [
    "def create_file(client, file_path):\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        # Download the file content from the URL\n",
    "        response = requests.get(file_path)\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        file_tuple = (file_name, file_content)\n",
    "        result = client.files.create(file=file_tuple, purpose=\"assistants\")\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        with open(file_path, \"rb\") as file_content:\n",
    "            result = client.files.create(file=file_content, purpose=\"assistants\")\n",
    "    print(result.id)\n",
    "    return result.id\n",
    "\n",
    "\n",
    "# Replace with your own file path or URL\n",
    "file_id = create_file(client, file_name)\n",
    "\n",
    "# Create a vector store\n",
    "vector_store = client.vector_stores.create(name=\"knowledge_base\")\n",
    "print(vector_store.id)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Add the file to the vector store\n",
    "client.vector_stores.files.create(vector_store_id=vector_store.id, file_id=file_id)\n",
    "\n",
    "result = client.vector_stores.files.list(vector_store_id=vector_store.id)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to 07-16-2025_lumbar facet joint_summary.md\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a skilled and detail oriented analyst with a background in data interpretation and technical writing. You have a talent for identifying patterns and extracting meaningful insights from research data, then communicating those insights effectively and succinctly through well-crafted reports. You are responsible for creating digest documents for a medical team that summarizes new findings for a given medical topic.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Create a summary document titled 'What's new in {topic}'. Each paper title is numbered in the document you receive. Make sure you summarize every single research paper, do not skip any papers. The summary document should be formatted as follows:\n",
    "            # What's new in {topic}\n",
    "    ## Randomized Controlled Trials\n",
    "    ## Observational Studies (cohort studies, case-control studies, cross-sectional studies)\n",
    "    ## Basic Science Research\n",
    "    ## Meta-analyses\n",
    "    ## Systematic Reviews\n",
    "    ## Narrative Reviews\n",
    "    ## Case Reports and Case Series\n",
    "    ## Other\n",
    "    One line below each paper's summary, include a brief citation in the format of: \n",
    "    <em>Citation: Title, Journal, PubDate, PMID: [PMID] (URL)</em>\n",
    "    Then, please rank all of the papers in order of highest impact to lowest impact. Give a 1 sentence explanation for each paper as well, explaining why it is ranked that way. This should be formatted as:\n",
    "    Paper title - One sentence justification for ranking.\n",
    "    \n",
    "    \"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is the content to summarize: \"\n",
    "            + open(\"PubMed_results.md\").read(),\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "# print(response.output_text)\n",
    "\n",
    "# Save the summary as a markdown file\n",
    "from datetime import datetime\n",
    "\n",
    "today = datetime.today().strftime(\"%m-%d-%Y\")\n",
    "filename = f\"{today}_{topic}_summary.md\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(response.output_text)\n",
    "\n",
    "print(f\"Summary saved to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
